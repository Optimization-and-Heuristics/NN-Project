{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from clases.DenseLayer import DenseLayer\n",
    "from clases.ConvLayer import ConvLayer\n",
    "from clases.FlattenLayer import FlattenLayer\n",
    "from clases.ActivationFunction import ReLU, Softmax\n",
    "from clases.LossFunction import cross_entropy_loss, cross_entropy_loss_grad\n",
    "from clases.Trainer import Trainer\n",
    "from clases.Optimizer import GDOptimizer, AdamOptimizer\n",
    "from clases.NeuronalNetwork import NeuronalNetwork\n",
    "\n",
    "from clases.Test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL: https://www.kaggle.com/datasets/animatronbot/mnist-digit-recognizer\n",
    "data = pd.read_csv('../datos/mnist_data.csv')\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = data.shape\n",
    "np.random.shuffle(data)\n",
    "\n",
    "test_size = int(0.1 * m)\n",
    "data_test = data[:test_size, :].T  # El 10% inicial para el conjunto de prueba\n",
    "\n",
    "# El conjunto provisional restante (90%)\n",
    "remaining_data = data[test_size:, :]\n",
    "\n",
    "train_size = int(0.8 * m)\n",
    "data_train = remaining_data[:train_size, :].T  # El siguiente 80% para entrenamiento\n",
    "data_validation = remaining_data[train_size:, :].T  # El resto 10% para validación\n",
    "\n",
    "X_train, Y_train = data_train[1:].T, data_train[0]\n",
    "X_val, Y_val = data_validation[1:].T, data_validation[0]\n",
    "X_test, Y_test = data_test[1:].T, data_test[0]\n",
    "\n",
    "Y_train = np.eye(10)[Y_train]\n",
    "Y_val = np.eye(10)[Y_val]\n",
    "Y_test = np.eye(10)[Y_test]\n",
    "\n",
    "# Normalización\n",
    "X_train = X_train / 255\n",
    "X_val = X_val / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "#Reshape\n",
    "X_train = X_train.reshape(len(X_train), 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [\n",
    "    ConvLayer((1,28,28),3,5),\n",
    "    FlattenLayer((5,26,26), (1,5*26*26)),\n",
    "    DenseLayer(5 * 26 * 26, 100, ReLU),\n",
    "    DenseLayer(100, 10, Softmax)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20, error=0.344375153913375\n",
      "2/20, error=0.2935703535872778\n",
      "3/20, error=0.2774450412775601\n",
      "4/20, error=0.26762219961211303\n",
      "5/20, error=0.2614077211530698\n",
      "6/20, error=0.25723308127150596\n",
      "7/20, error=0.25410305554776785\n",
      "8/20, error=0.25152565317872155\n",
      "9/20, error=0.24926453238759125\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for e in range(epochs):\n",
    "    error = 0\n",
    "    for x, y in zip(X_train, Y_train):\n",
    "        output = x\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "\n",
    "        error += cross_entropy_loss(y, output)\n",
    "\n",
    "        grad = cross_entropy_loss_grad(y, output)\n",
    "        for layer in reversed(network):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "    error /= len(X_train)\n",
    "    print(f\"{e+1}/{epochs}, error={error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = predict(nn, X_test)\n",
    "accuracy, metrics = calculate_metrics(Y_pred, Y_test)\n",
    "confusion_matrix(Y_test, Y_pred, class_labels=range(10))\n",
    "print_metrics(accuracy,metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m GDOptimizer(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(nn, optimizer, cross_entropy_loss, cross_entropy_loss_grad)\n\u001b[1;32m----> 5\u001b[0m loss, acc, val_info \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Radka\\Desktop\\NN-Project\\code\\clases\\Trainer.py:24\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, X_train, Y_train, X_val, Y_val, epochs, print_every)\u001b[0m\n\u001b[0;32m     22\u001b[0m acc\u001b[38;5;241m.\u001b[39mappend(evaluate(predictions, Y_train))\n\u001b[0;32m     23\u001b[0m grad_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function_grad(Y_train, predictions)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(layer, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m: layer\u001b[38;5;241m.\u001b[39mgrad_weights, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiases\u001b[39m\u001b[38;5;124m\"\u001b[39m: layer\u001b[38;5;241m.\u001b[39mgrad_biases}, \n\u001b[0;32m     27\u001b[0m                           t\u001b[38;5;241m=\u001b[39mepoch)\n",
      "File \u001b[1;32mc:\\Users\\Radka\\Desktop\\NN-Project\\code\\clases\\NeuronalNetwork.py:23\u001b[0m, in \u001b[0;36mNeuronalNetwork.backward\u001b[1;34m(self, grad_output)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, grad_output):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Algorithm 4. Backward Pass con Múltiples Capas\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m---> 23\u001b[0m         grad_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Radka\\Desktop\\NN-Project\\code\\clases\\DenseLayer.py:20\u001b[0m, in \u001b[0;36mDenseLayer.backward\u001b[1;34m(self, grad_output)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, grad_output):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Algorithm 5. Backward Pass para Dense Layer\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     grad_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput\u001b[38;5;241m.\u001b[39mT, grad_output)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_biases \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(grad_output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Radka\\Desktop\\NN-Project\\code\\clases\\ActivationFunction.py:28\u001b[0m, in \u001b[0;36mSoftmax.backward\u001b[1;34m(self, output, grad_output)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, gradient \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(grad_output):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m---> 28\u001b[0m     jacobian_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiagflat(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(output[i][:, \u001b[38;5;28;01mNone\u001b[39;00m], output[i][\u001b[38;5;28;01mNone\u001b[39;00m, :])\n\u001b[0;32m     29\u001b[0m     gradients\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mdot(jacobian_matrix, gradient))\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(gradients)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "nn = NeuronalNetwork(None, [])\n",
    "nn.layers = network\n",
    "optimizer = GDOptimizer(learning_rate=0.1)\n",
    "trainer = Trainer(nn, optimizer, cross_entropy_loss, cross_entropy_loss_grad)\n",
    "loss, acc, val_info = trainer.train(X_train, Y_train,X_val,Y_val, epochs=20, print_every=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
